# Copyright (c) 2025, Istituto Italiano di Tecnologia
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause


from __future__ import annotations

import os
import statistics
import time
from collections import deque

import torch
from torch.utils.tensorboard import SummaryWriter as TensorboardSummaryWriter

import rsl_rl
from rsl_rl.env import VecEnv
from rsl_rl.modules import ActorCritic, ActorCriticRecurrent
from rsl_rl.utils import resolve_obs_groups

from rsl_rl.utils import AMPLoader
from rsl_rl.algorithms import AMP_PPO
from rsl_rl.networks import Discriminator, ActorCriticMoE
from rsl_rl.utils import export_policy_as_onnx
from rsl_rl.utils.logger import Logger

class AMPOnPolicyRunner:
    """
    AMPOnPolicyRunner æ˜¯ä¸€ä¸ªé«˜çº§åè°ƒå™¨ï¼Œç”¨äºç®¡ç†ä½¿ç”¨å¯¹æŠ—è¿åŠ¨å…ˆéªŒ (AMP) 
    ç»“åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹  (PPO) çš„ç­–ç•¥è®­ç»ƒå’Œè¯„ä¼°ã€‚

    å®ƒæ•´åˆäº†å¤šä¸ªç»„ä»¶ï¼š
    - ç¯å¢ƒ (`VecEnv`)
    - ç­–ç•¥ (`ActorCritic`, `ActorCriticRecurrent`)
    - åˆ¤åˆ«å™¨ (Discriminator)
    - ä¸“å®¶æ•°æ®é›† (AMPLoader)
    - å¥–åŠ±ç»„åˆ (ä»»åŠ¡å¥–åŠ± + é£æ ¼å¥–åŠ±)
    - æ—¥å¿—è®°å½•å’Œæ£€æŸ¥ç‚¹ä¿å­˜

    ---
    ğŸ”§ é…ç½® Configuration
    ----------------
    è¯¥ç±»æœŸæœ›ä¸€ä¸ª `train_cfg` å­—å…¸ï¼Œå…¶ç»“æ„åŒ…å«ä»¥ä¸‹é”®ï¼š
    - "obs_groups": å¯é€‰æ˜ å°„ï¼Œæè¿°å“ªäº›è§‚æµ‹å¼ é‡å±äº "policy" è¾“å…¥ï¼Œå“ªäº›å±äº "critic" è¾“å…¥ã€‚
    - "policy": ç­–ç•¥ç½‘ç»œçš„é…ç½®ï¼ŒåŒ…æ‹¬ `"class_name"`ã€‚
    - "algorithm": PPO/AMP_PPO çš„é…ç½®ï¼ŒåŒ…æ‹¬ `"class_name"`ã€‚
    - "discriminator": AMP åˆ¤åˆ«å™¨çš„é…ç½®ã€‚
    - "dataset": ä¼ é€’ç»™ `AMPLoader` çš„å­—å…¸ï¼Œè‡³å°‘åŒ…å«ï¼š
        * "amp_data_path": å­˜æ”¾ `.npy` ä¸“å®¶æ•°æ®é›†çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
        * "datasets": æ•°æ®é›†åç§° -> é‡‡æ ·æƒé‡ (float) çš„æ˜ å°„ã€‚
        * "slow_down_factor": åº”ç”¨äºçœŸå®è¿åŠ¨æ•°æ®çš„å‡é€Ÿå› å­ï¼Œä»¥åŒ¹é…ä»¿çœŸåŠ¨åŠ›å­¦ã€‚
    - "num_steps_per_env": æ¯ä¸ªç¯å¢ƒçš„ Rollout è§†ç•Œé•¿åº¦ (horizon)ã€‚
    - "save_interval": æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ï¼ˆä»¥è¿­ä»£æ¬¡æ•°è®¡ï¼‰ã€‚
    - "empirical_normalization": (å·²å¼ƒç”¨) é•œåƒåˆ° `policy.actor_obs_normalization` çš„æ—§æ ‡å¿—ã€‚
    - "logger": "tensorboard", "wandb", æˆ– "neptune" ä¹‹ä¸€ã€‚

    ---
    ğŸ“¦ æ•°æ®é›†æ ¼å¼ Dataset format
    ------------------
    é€šè¿‡ `AMPLoader` åŠ è½½çš„ä¸“å®¶è¿åŠ¨æ•°æ®é›†å¿…é¡»æ˜¯åŒ…å«å­—å…¸çš„ `.npy` æ–‡ä»¶ï¼š

    - `"joints_list"`: List[str] â€” æœ‰åºçš„å…³èŠ‚åç§°åˆ—è¡¨ã€‚
    - `"joint_positions"`: List[np.ndarray] â€” æ¯ä¸ªæ—¶é—´æ­¥çš„å…³èŠ‚é…ç½® (1D æ•°ç»„)ã€‚
    - `"root_position"`: List[np.ndarray] â€” ä¸–ç•Œåæ ‡ç³»ä¸‹çš„åŸºåº§ä½ç½®ã€‚
    - `"root_quaternion"`: List[np.ndarray] â€” **`xyzw`** æ ¼å¼çš„åŸºåº§æ–¹å‘ (SciPy é»˜è®¤)ã€‚
    - `"fps"`: float â€” åŸå§‹æ•°æ®é›†å¸§ç‡ã€‚

    å†…éƒ¨å¤„ç†ï¼š
    - å››å…ƒæ•°é€šè¿‡ SLERP æ’å€¼å¹¶åœ¨ä½¿ç”¨å‰è½¬æ¢ä¸º **`wxyz`** æ ¼å¼ (ä»¥åŒ¹é… Isaac Gym æƒ¯ä¾‹)ã€‚
    - é€Ÿåº¦é€šè¿‡æœ‰é™å·®åˆ†ä¼°ç®—ã€‚
    - æ‰€æœ‰æ•°æ®è½¬æ¢ä¸º torch tensors å¹¶æ”¾ç½®åœ¨æŒ‡å®šè®¾å¤‡ä¸Šã€‚

    ---
    ğŸ“ AMP å¥–åŠ± AMP Reward
    -------------
    åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œrunner æ”¶é›† AMP ç‰¹å®šçš„è§‚æµ‹ï¼Œå¹¶ä»ä¸“å®¶æ•°æ®é›†ä¸­è®¡ç®—
    åŸºäºåˆ¤åˆ«å™¨çš„â€œé£æ ¼å¥–åŠ±â€ã€‚è¯¥å¥–åŠ±ä¸ç¯å¢ƒå¥–åŠ±ç»“åˆå¦‚ä¸‹ï¼š

        `reward = 0.5 * task_reward + 0.5 * style_reward`
    
    (æ³¨æ„ï¼šä»£ç å®é™…å®ç°ä¸­é€šå¸¸æ˜¯åœ¨ Env æˆ– Wrapper é‡Œåšæ··åˆï¼Œæˆ–è€…åƒè¿™é‡Œä¸€æ ·åœ¨ Runner çš„ step å¾ªç¯é‡Œæ··åˆ)

    ---
    ğŸ” è®­ç»ƒå¾ªç¯ Training loop
    ----------------
    `learn()` æ–¹æ³•æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
    - `rollout`: é€šè¿‡ `self.alg.act()` å’Œ `env.step()` æ”¶é›† TensorDict è§‚æµ‹ã€‚
    - `style_reward`: é€šè¿‡åˆ¤åˆ«å™¨ `predict_reward(...)` è®¡ç®—ã€‚
    - `storage update`: é€šè¿‡ `process_env_step()` å’Œ `process_amp_step()` æ›´æ–°å­˜å‚¨ã€‚
    - `return computation`: ä½¿ç”¨æœ€æ–°çš„ TensorDict è§‚æµ‹é€šè¿‡ `compute_returns()` è®¡ç®—å›æŠ¥ã€‚
    - `update`: ä½¿ç”¨ `self.alg.update()` æ‰§è¡Œåå‘ä¼ æ’­ã€‚
    - é€šè¿‡ TensorBoard/WandB/Neptune è®°å½•æ—¥å¿—ã€‚

    ---
    ğŸ’¾ ä¿å­˜å’Œ ONNX å¯¼å‡º Saving and ONNX export
    --------------------------
    åœ¨æ¯ä¸ª `save_interval`ï¼Œrunner ä¼šï¼š
    - ä¿å­˜å®Œæ•´çŠ¶æ€ (`model`, `optimizer`, `discriminator`, ç­‰)ã€‚
    - å¯é€‰åœ°å°†ç­–ç•¥å¯¼å‡ºä¸º ONNX æ¨¡å‹ç”¨äºéƒ¨ç½²ã€‚
    - å¦‚æœå¯ç”¨ï¼Œå°†æ£€æŸ¥ç‚¹ä¸Šä¼ åˆ°æ—¥å¿—æœåŠ¡ã€‚

    """

    def __init__(self, env: VecEnv, train_cfg, log_dir=None, device="cpu"):
        # åˆå§‹åŒ–é…ç½®
        self.cfg = train_cfg
        self.alg_cfg = train_cfg["algorithm"]
        self.policy_cfg = train_cfg["policy"]
        self.discriminator_cfg = train_cfg["discriminator"]
        self.dataset_cfg = train_cfg["dataset"]
        self.device = device
        self.env = env

        # è·å–ç¯å¢ƒçš„åˆå§‹è§‚æµ‹ï¼ˆç”¨äºç¡®å®šç»´åº¦ï¼‰
        observations = self.env.get_observations()
        default_sets = ["critic"]
        # è§£æè§‚æµ‹åˆ†ç»„ (Policy è¾“å…¥ vs Critic è¾“å…¥)
        self.cfg["obs_groups"] = resolve_obs_groups(
            observations, self.cfg.get("obs_groups"), default_sets
        )

        self.alg: AMP_PPO = self._construct_algorithm(observations)
        
        self.num_steps_per_env = self.cfg["num_steps_per_env"]
        self.save_interval = self.cfg["save_interval"]
        
        # åˆå§‹åŒ– Storage (Rollout Buffer)
        obs_template = observations.clone().detach().to(self.device)
        self.alg.init_storage(
            self.env.num_envs,
            self.num_steps_per_env,
            obs_template,
            (self.env.num_actions,),
        )

        # æ—¥å¿—ç›¸å…³åˆå§‹åŒ–
        self.log_dir = log_dir
        self.writer = None
        self.logger_type = None
        self.tot_timesteps = 0
        self.tot_time = 0
        self.current_learning_iteration = 0
        # Create the logger
        self.logger = Logger(
            log_dir=log_dir,
            cfg=self.cfg,
            env_cfg=self.env.cfg,
            num_envs=self.env.num_envs,
            is_distributed=self.is_distributed,
            gpu_world_size=self.gpu_world_size,
            gpu_global_rank=self.gpu_global_rank,
            device=self.device,
        )

    def learn(self, num_learning_iterations: int, init_at_random_ep_len: bool = False):
        """æ‰§è¡Œä¸»è®­ç»ƒå¾ªç¯ã€‚"""
        
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ (Writer)
        if self.log_dir is not None and self.writer is None:
            # é»˜è®¤ä½¿ç”¨ Tensorboardï¼Œä¹Ÿæ”¯æŒ Neptune å’Œ WandB
            self.logger_type = self.cfg.get("logger", "tensorboard")
            self.logger_type = self.logger_type.lower()

            if self.logger_type == "neptune":
                # Neptune Logger åˆå§‹åŒ–
                from rsl_rl.utils.neptune_utils import NeptuneSummaryWriter
                self.writer = NeptuneSummaryWriter(
                    log_dir=self.log_dir, flush_secs=10, cfg=self.cfg
                )
                self.writer.log_config(
                    self.env.cfg, self.cfg, self.alg_cfg, self.policy_cfg
                )
            elif self.logger_type == "wandb":
                # WandB Logger åˆå§‹åŒ–
                from rsl_rl.utils.wandb_utils import WandbSummaryWriter
                import wandb

                # ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºç»™ run name åŠ ä¸Šè‡ªåŠ¨é€’å¢çš„åºå·åç¼€ (æ¨¡æ‹Ÿ rsl-rl æ—§ç‰ˆè¡Œä¸º)
                def update_run_name_with_sequence(prefix: str) -> None:
                    project = wandb.run.project
                    entity = wandb.run.entity
                    api = wandb.Api()
                    runs = api.runs(f"{entity}/{project}")
                    max_num = 0
                    for run in runs:
                        if run.name.startswith(prefix):
                            numeric_suffix = run.name[len(prefix) :]
                            try:
                                run_num = int(numeric_suffix)
                                if run_num > max_num:
                                    max_num = run_num
                            except ValueError:
                                continue
                    new_num = max_num + 1
                    new_run_name = f"{prefix}{new_num}"
                    wandb.run.name = new_run_name
                    print("Updated run name to:", wandb.run.name)

                self.writer = WandbSummaryWriter(
                    log_dir=self.log_dir, flush_secs=10, cfg=self.cfg
                )
                update_run_name_with_sequence(prefix=self.cfg["wandb_project"])
                wandb.gym.monitor()
                self.writer.log_config(
                    self.env.cfg, self.cfg, self.alg_cfg, self.policy_cfg
                )
            elif self.logger_type == "tensorboard":
                # Tensorboard Logger åˆå§‹åŒ–
                self.writer = TensorboardSummaryWriter(
                    log_dir=self.log_dir, flush_secs=10
                )
            else:
                raise AssertionError("logger type not found")

        # éšæœºåŒ–åˆå§‹å›åˆé•¿åº¦ (é˜²æ­¢æ‰€æœ‰ç¯å¢ƒåŒæ­¥ Reset)
        if init_at_random_ep_len:
            self.env.episode_length_buf = torch.randint_like(
                self.env.episode_length_buf, high=int(self.env.max_episode_length)
            )
            
        # è·å–åˆå§‹è§‚æµ‹
        obs = self.env.get_observations().to(self.device)
        # è·å–åˆå§‹ AMP è§‚æµ‹ (ç”¨äºè®¡ç®— reward, éœ€è¦ä¸Šä¸€å¸§å’Œå½“å‰å¸§)
        amp_obs = obs["amp"].clone()
        
        self.train_mode()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼

        # ç»Ÿè®¡æ•°æ®ç¼“å­˜
        ep_infos = []
        rewbuffer = deque(maxlen=100)
        lenbuffer = deque(maxlen=100)
        cur_reward_sum = torch.zeros(
            self.env.num_envs, dtype=torch.float, device=self.device
        )
        cur_episode_length = torch.zeros(
            self.env.num_envs, dtype=torch.float, device=self.device
        )

        start_iter = self.current_learning_iteration
        tot_iter = start_iter + num_learning_iterations
        
        # >>> ä¸»å¾ªç¯å¼€å§‹ <<<
        for it in range(start_iter, tot_iter):
            start = time.time()
            
            # --- 1. Rollout (æ•°æ®æ”¶é›†) ---

            mean_style_reward_log = 0
            mean_task_reward_log = 0

            with torch.inference_mode(): # ä¸è®¡ç®—æ¢¯åº¦
                for _ in range(self.num_steps_per_env): # æ¯æ¬¡ rollout æ”¶é›† N æ­¥
                    # 1. ç­–ç•¥ç½‘ç»œè¾“å‡ºåŠ¨ä½œ
                    actions = self.alg.act(obs)
                    # è®°å½•å½“å‰ AMP è§‚æµ‹ (state t)
                    self.alg.act_amp(amp_obs)
                    
                    # 2. ç¯å¢ƒæ­¥è¿›
                    obs, rewards, dones, extras = self.env.step(
                        actions.to(self.env.device)
                    )
                    obs = obs.to(self.device)
                    rewards = rewards.to(self.device)
                    dones = dones.to(self.device)

                    # è·å–ä¸‹ä¸€å¸§ AMP è§‚æµ‹ (state t+1)
                    next_amp_obs = obs["amp"].clone()
                    
                    # 3. è®¡ç®—é£æ ¼å¥–åŠ± (Style Reward)
                    # è¾“å…¥æ˜¯ (s_t, s_{t+1}) å¯¹
                    style_rewards = self.discriminator.predict_reward(
                        amp_obs, next_amp_obs
                    )

                    # è®°å½•å¥–åŠ±ç»Ÿè®¡
                    mean_task_reward_log += rewards.mean().item()
                    mean_style_reward_log += style_rewards.mean().item()

                    # 4. æ··åˆå¥–åŠ±: Task Reward + Style Reward
                    # ç³»æ•° 0.5 æ˜¯ç¡¬ç¼–ç çš„ï¼Œå¯ä»¥æå–åˆ° config ä¸­
                    rewards = 0.5 * rewards + 0.5 * style_rewards

                    # 5. å¤„ç†æ•°æ®å¹¶å­˜å…¥ PPO Storage
                    self.alg.process_env_step(obs, rewards, dones, extras)
                    # å­˜å…¥ AMP Replay Buffer
                    self.alg.process_amp_step(next_amp_obs)

                    # æ›´æ–° amp_obs ä¸ºå½“å‰å¸§ï¼Œä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨
                    amp_obs = next_amp_obs

                    # 6. å¤„ç†æ—¥å¿—ä¿¡æ¯ (Episode ç»“æŸæ—¶çš„ç»Ÿè®¡)
                    if self.log_dir is not None:
                        if "episode" in extras:
                            ep_infos.append(extras["episode"])
                        elif "log" in extras:
                            ep_infos.append(extras["log"])
                        
                        cur_reward_sum += rewards
                        cur_episode_length += 1
                        
                        # å¤„ç† Done çš„ç¯å¢ƒ
                        new_ids = torch.nonzero(dones, as_tuple=False)
                        if new_ids.numel() > 0:
                            env_indices = new_ids.view(-1)
                            rewbuffer.extend(cur_reward_sum[env_indices].cpu().tolist())
                            lenbuffer.extend(
                                cur_episode_length[env_indices].cpu().tolist()
                            )
                            # é‡ç½®ç»Ÿè®¡å™¨
                            cur_reward_sum[env_indices] = 0
                            cur_episode_length[env_indices] = 0

                stop = time.time()
                collection_time = stop - start

                # --- 2. Learning Step (å­¦ä¹ æ­¥éª¤) ---
                start = stop
                # è®¡ç®— GAE å›æŠ¥
                self.alg.compute_returns(obs)

            # å½’ä¸€åŒ– log æ•°æ®
            mean_style_reward_log /= self.num_steps_per_env
            mean_task_reward_log /= self.num_steps_per_env

            # æ‰§è¡Œ PPO + AMP æ›´æ–°
            (
                mean_value_loss,
                mean_surrogate_loss,
                mean_amp_loss,
                mean_grad_pen_loss,
                mean_policy_pred,
                mean_expert_pred,
                mean_accuracy_policy,
                mean_accuracy_expert,
                mean_kl_divergence,
            ) = self.alg.update()
            
            stop = time.time()
            learn_time = stop - start
            self.current_learning_iteration = it
            
            # --- 3. Logging & Saving (æ—¥å¿—ä¸ä¿å­˜) ---
            if self.log_dir is not None:
                self.log(locals()) # æ‰“å°å’Œå†™å…¥ TensorBoard
            
            if it % self.save_interval == 0:
                self.save(os.path.join(self.log_dir, f"model_{it}.pt"), save_onnx=True)
            
            ep_infos.clear()
            
            # ä¿å­˜ git çŠ¶æ€ (ä»…ç¬¬ä¸€æ¬¡è¿­ä»£)
            if it == start_iter:
                pass
                # git_file_paths = store_code_state(self.log_dir, self.git_status_repos)
                # if self.logger_type in ["wandb", "neptune"] and git_file_paths:
                #     for path in git_file_paths:
                #         self.writer.save_file(path)

        # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save(
            os.path.join(self.log_dir, f"model_{self.current_learning_iteration}.pt"),
            save_onnx=True,
        )

    def log(self, locs: dict, width: int = 80, pad: int = 35):
        """å¤„ç†æ—¥å¿—æ‰“å°åˆ°æ§åˆ¶å°å’Œå†™å…¥ Loggerã€‚"""
        self.tot_timesteps += self.num_steps_per_env * self.env.num_envs
        self.tot_time += locs["collection_time"] + locs["learn_time"]
        iteration_time = locs["collection_time"] + locs["learn_time"]

        ep_string = ""
        # å¤„ç† Env è¿”å›çš„é¢å¤–ä¿¡æ¯ (extras['episode'])
        if locs["ep_infos"]:
            for key in locs["ep_infos"][0]:
                infotensor = torch.tensor([], device=self.device)
                for ep_info in locs["ep_infos"]:
                    if key not in ep_info: continue
                    if not isinstance(ep_info[key], torch.Tensor):
                        ep_info[key] = torch.Tensor([ep_info[key]])
                    if len(ep_info[key].shape) == 0:
                        ep_info[key] = ep_info[key].unsqueeze(0)
                    infotensor = torch.cat((infotensor, ep_info[key].to(self.device)))
                
                value = torch.mean(infotensor)
                # å†™å…¥ Writer
                if "/" in key:
                    self.writer.add_scalar(key, value, locs["it"])
                    ep_string += f"""{f'{key}:':>{pad}} {value:.4f}\n"""
                else:
                    self.writer.add_scalar("Episode/" + key, value, locs["it"])
                    ep_string += f"""{f'Mean episode {key}:':>{pad}} {value:.4f}\n"""
        
        # è·å– Action Noise Std ç”¨äºç›‘æ§æ¢ç´¢ç¨‹åº¦
        if getattr(self.alg.actor_critic, "noise_std_type", "scalar") == "log":
            mean_std_value = torch.exp(self.alg.actor_critic.log_std).mean()
        else:
            mean_std_value = self.alg.actor_critic.std.mean()
            
        fps = int(
            self.num_steps_per_env
            * self.env.num_envs
            / (locs["collection_time"] + locs["learn_time"])
        )

        # å†™å…¥æ ‡å‡† PPO æŸå¤±
        self.writer.add_scalar("Loss/value_function", locs["mean_value_loss"], locs["it"])
        self.writer.add_scalar("Loss/surrogate", locs["mean_surrogate_loss"], locs["it"])

        # å†™å…¥ AMP ç›¸å…³æŒ‡æ ‡ (Loss, Prediction, Accuracy)
        self.writer.add_scalar("Loss/amp_loss", locs["mean_amp_loss"], locs["it"])
        self.writer.add_scalar("Loss/grad_pen_loss", locs["mean_grad_pen_loss"], locs["it"])
        self.writer.add_scalar("Loss/policy_pred", locs["mean_policy_pred"], locs["it"])
        self.writer.add_scalar("Loss/expert_pred", locs["mean_expert_pred"], locs["it"])
        self.writer.add_scalar("Loss/accuracy_policy", locs["mean_accuracy_policy"], locs["it"])
        self.writer.add_scalar("Loss/accuracy_expert", locs["mean_accuracy_expert"], locs["it"])

        # å†™å…¥å­¦ä¹ ç‡å’Œ KL æ•£åº¦
        self.writer.add_scalar("Loss/learning_rate", self.alg.learning_rate, locs["it"])
        self.writer.add_scalar("Loss/mean_kl_divergence", locs["mean_kl_divergence"], locs["it"])
        self.writer.add_scalar("Policy/mean_noise_std", mean_std_value.item(), locs["it"])
        
        # å†™å…¥æ€§èƒ½æŒ‡æ ‡ (FPS, Time)
        self.writer.add_scalar("Perf/total_fps", fps, locs["it"])
        self.writer.add_scalar("Perf/collection time", locs["collection_time"], locs["it"])
        self.writer.add_scalar("Perf/learning_time", locs["learn_time"], locs["it"])
        
        # å†™å…¥è®­ç»ƒå¥–åŠ± (Reward Buffer ç»Ÿè®¡)
        if len(locs["rewbuffer"]) > 0:
            self.writer.add_scalar("Train/mean_reward", statistics.mean(locs["rewbuffer"]), locs["it"])
            self.writer.add_scalar("Train/mean_episode_length", statistics.mean(locs["lenbuffer"]), locs["it"])
            self.writer.add_scalar("Train/mean_style_reward", locs["mean_style_reward_log"], locs["it"])
            self.writer.add_scalar("Train/mean_task_reward", locs["mean_task_reward_log"], locs["it"])
            
            if self.logger_type != "wandb":
                self.writer.add_scalar("Train/mean_reward/time", statistics.mean(locs["rewbuffer"]), self.tot_time)
                self.writer.add_scalar("Train/mean_episode_length/time", statistics.mean(locs["lenbuffer"]), self.tot_time)

        # æ„å»ºæ‰“å°å­—ç¬¦ä¸²
        str = f" \033[1m Learning iteration {locs['it']}/{locs['tot_iter']} \033[0m "
        
        log_string = (
            f"""{'#' * width}\n"""
            f"""{str.center(width, ' ')}\n\n"""
            f"""{'Computation:':>{pad}} {fps:.0f} steps/s (collection: {locs['collection_time']:.3f}s, learning {locs['learn_time']:.3f}s)\n"""
            f"""{'Value function loss:':>{pad}} {locs['mean_value_loss']:.4f}\n"""
            f"""{'Surrogate loss:':>{pad}} {locs['mean_surrogate_loss']:.4f}\n"""
            f"""{'Mean action noise std:':>{pad}} {mean_std_value.item():.2f}\n"""
        )
        
        if len(locs["rewbuffer"]) > 0:
            log_string += (
                f"""{'Mean reward:':>{pad}} {statistics.mean(locs['rewbuffer']):.2f}\n"""
                f"""{'Mean episode length:':>{pad}} {statistics.mean(locs['lenbuffer']):.2f}\n"""
            )

        log_string += ep_string

        # è®¡ç®— ETA
        eta_seconds = (
            self.tot_time
            / (locs["it"] + 1)
            * (locs["num_learning_iterations"] - locs["it"])
        )
        eta_h, rem = divmod(eta_seconds, 3600)
        eta_m, eta_s = divmod(rem, 60)

        log_string += (
            f"""{'-' * width}\n"""
            f"""{'Total timesteps:':>{pad}} {self.tot_timesteps}\n"""
            f"""{'Iteration time:':>{pad}} {iteration_time:.2f}s\n"""
            f"""{'Total time:':>{pad}} {self.tot_time:.2f}s\n"""
            f"""{'ETA:':>{pad}} {int(eta_h)}h {int(eta_m)}m {int(eta_s)}s\n"""
        )
        print(log_string)

    def save(self, path, infos=None, save_onnx=False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
        saved_dict = {
            "model_state_dict": self.alg.actor_critic.state_dict(),
            "optimizer_state_dict": self.alg.optimizer.state_dict(),
            "discriminator_state_dict": self.alg.discriminator.state_dict(),
            "iter": self.current_learning_iteration,
            "infos": infos,
        }
        torch.save(saved_dict, path)

        if self.logger_type in ["neptune", "wandb"]:
            self.writer.save_model(path, self.current_learning_iteration)

        if save_onnx:
            # å¯¼å‡º ONNX
            onnx_folder = os.path.dirname(path)
            iteration = int(os.path.basename(path).split("_")[1].split(".")[0])
            onnx_model_name = f"policy_{iteration}.onnx"

            export_policy_as_onnx(
                self.alg.actor_critic,
                normalizer=self.alg.actor_critic.actor_obs_normalizer,
                path=onnx_folder,
                filename=onnx_model_name,
            )

            if self.logger_type in ["neptune", "wandb"]:
                self.writer.save_model(
                    os.path.join(onnx_folder, onnx_model_name),
                    self.current_learning_iteration,
                )

    def load(self, path, load_optimizer=True, weights_only=False):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
        loaded_dict = torch.load(
            path, map_location=self.device, weights_only=weights_only
        )
        self.alg.actor_critic.load_state_dict(loaded_dict["model_state_dict"])
        
        # åŠ è½½åˆ¤åˆ«å™¨
        discriminator_state = loaded_dict["discriminator_state_dict"]
        self.alg.discriminator.load_state_dict(discriminator_state, strict=False)

        # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šåŠ è½½åˆ†ç¦»çš„ Normalizer
        amp_normalizer_module = loaded_dict.get("amp_normalizer")
        if amp_normalizer_module is not None and getattr(
            self.alg.discriminator, "empirical_normalization", False
        ):
            self.alg.discriminator.amp_normalizer.load_state_dict(
                amp_normalizer_module.state_dict()
            )
            
        if load_optimizer:
            self.alg.optimizer.load_state_dict(loaded_dict["optimizer_state_dict"])
            
        self.current_learning_iteration = loaded_dict["iter"]
        return loaded_dict["infos"]

    def get_inference_policy(self, device=None):
        """è·å–ç”¨äºæ¨ç†çš„ç­–ç•¥å‡½æ•°ã€‚"""
        self.eval_mode()
        if device is not None:
            self.alg.actor_critic.to(device)
        return self.alg.actor_critic.act_inference

    def train_mode(self):
        """è®¾ç½®å…¨æ¨¡å—ä¸ºè®­ç»ƒæ¨¡å¼ã€‚"""
        self.alg.actor_critic.train()
        self.alg.discriminator.train()

    def eval_mode(self):
        """è®¾ç½®å…¨æ¨¡å—ä¸ºè¯„ä¼°æ¨¡å¼ã€‚"""
        self.alg.actor_critic.eval()
        self.alg.discriminator.eval()

    def add_git_repo_to_log(self, repo_file_path: str) -> None:
        self.logger.git_status_repos.append(repo_file_path)
    
    def _construct_algorithm(self, observations: TensorDict) -> AMP_PPO:
                # Create the algorithm
        # åŠ¨æ€åŠ è½½å¹¶å®ä¾‹åŒ– Policy ç±» (ActorCritic, ActorCriticRecurrent, MoE)
        actor_critic_class = eval(self.policy_cfg.pop("class_name"))  # e.g., ActorCritic
        actor_critic: ActorCritic | ActorCriticRecurrent | ActorCriticMoE = (
            actor_critic_class(
                observations,
                self.cfg["obs_groups"],
                self.env.num_actions,
                **self.policy_cfg,
            ).to(self.device)
        )
        
        # NOTE: ä¸ºäº†ä½¿ç”¨ AMPï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ç¯å¢ƒçš„è§‚æµ‹é…ç½®ä¸ AMP è§‚æµ‹ä¸€è‡´ã€‚
        # è¿™é‡Œä» Isaac Lab çš„é…ç½®ä¸­æå– AMP å…³èŠ‚åç§°ã€‚
        # æ³¨æ„ï¼šè¿™è¡Œä»£ç å¼ºä¾èµ–äº Manager-Based ç¯å¢ƒç»“æ„ã€‚
        # amp_joint_names = self.env.cfg.observations.amp.joint_pos.params[
        #     "asset_cfg"
        # ].joint_names

        # --- [ä¿®æ”¹å¼€å§‹] å…¼å®¹ Direct å’Œ Manager Based ç¯å¢ƒ ---
        
        # å°è¯•æ–¹æ³• 1: Direct ç¯å¢ƒé€šå¸¸åœ¨ cfg ä¸­ç›´æ¥å­˜æœ‰å…³èŠ‚åç§°åˆ—è¡¨ (å–å†³äºä½ æ€ä¹ˆå†™çš„ Config)
        if hasattr(self.env.unwrapped, "cfg") and hasattr(self.env.unwrapped.cfg, "dof_names"):
             amp_joint_names = self.env.unwrapped.cfg.dof_names
        
        # å°è¯•æ–¹æ³• 2: Direct ç¯å¢ƒå·²ç»åˆå§‹åŒ–äº† robotï¼Œå¯ä»¥ç›´æ¥ä» robot å®ä¾‹è·å–
        elif hasattr(self.env.unwrapped, "robot") and hasattr(self.env.unwrapped.robot.data, "joint_names"):
            # æ³¨æ„ï¼šDirectRLEnv çš„ robot.data.joint_names é€šå¸¸æ˜¯æ‰€æœ‰å…³èŠ‚
            # å¦‚æœ AMP åªéœ€è¦ä¸€éƒ¨åˆ†ï¼Œä½ éœ€è¦æ‰‹åŠ¨æŒ‡å®šã€‚
            # è¿™é‡Œå‡è®¾æˆ‘ä»¬éœ€è¦æ‰€æœ‰é©±åŠ¨å…³èŠ‚
            amp_joint_names = self.env.unwrapped.robot.data.joint_names
            
        # å°è¯•æ–¹æ³• 3: Manager Based ç¯å¢ƒ (åŸæœ‰çš„é€»è¾‘)
        elif hasattr(self.env.cfg, "observations") and hasattr(self.env.cfg.observations, "amp"):
            amp_joint_names = self.env.cfg.observations.amp.joint_pos.params["asset_cfg"].joint_names
            
        # å°è¯•æ–¹æ³• 4: ç¡¬ç¼–ç å›é€€ (å®åœ¨ä¸è¡Œï¼Œå°±åœ¨ Config é‡ŒåŠ ä¸€ä¸ªå­—æ®µ)
        elif "amp_joint_names" in self.dataset_cfg:
             amp_joint_names = self.dataset_cfg["amp_joint_names"]
             
        else:
            raise AttributeError(
                "Could not find joint names for AMPLoader. \n"
                "If using DirectRLEnv, please ensure 'dof_names' is in your env cfg or 'amp_joint_names' is in your train_cfg['dataset'].\n"
                "If using ManagerBasedRLEnv, ensure cfg.observations.amp structure exists."
            )

        # åˆå§‹åŒ– AMP æ‰€éœ€çš„æ‰€æœ‰é…æ–™ (åˆ¤åˆ«å™¨ï¼Œæ•°æ®é›†åŠ è½½å™¨)
        
        # å°è¯•ä» Environment å®ä¾‹ç›´æ¥è¯»å–å®šä¹‰å¥½çš„å°ºå¯¸
        if hasattr(self.env.unwrapped, "amp_observation_size"):
            num_amp_obs = self.env.unwrapped.amp_observation_size
        else:
            raise AttributeError("Cannot determine AMP observation size. Please define 'self.amp_observation_size' in your Env.")
        
        # åˆå§‹åŒ– AMPLoaderï¼šè´Ÿè´£åŠ è½½å’Œé‡‡æ ·ä¸“å®¶åŠ¨ä½œ
        amp_data = AMPLoader(
            device=self.device,
            dataset_path_root=self.dataset_cfg["amp_data_path"],
            datasets=self.dataset_cfg["datasets"],
            simulation_dt=self.env.cfg.sim.dt * self.env.cfg.decimation, # ç‰©ç†ä»¿çœŸæ­¥é•¿
            slow_down_factor=self.dataset_cfg["slow_down_factor"],
            expected_joint_names=amp_joint_names,
        )

        # åˆå§‹åŒ– Discriminatorï¼šåˆ¤åˆ«çœŸå‡åŠ¨ä½œ
        self.discriminator = Discriminator(
            input_dim=num_amp_obs * 2,  # åˆ¤åˆ«å™¨è¾“å…¥æ˜¯ (å½“å‰è§‚æµ‹ + ä¸‹ä¸€è§‚æµ‹) çš„æ‹¼æ¥ï¼Œæ‰€ä»¥ç»´åº¦ä¹˜ 2
            hidden_layer_sizes=self.discriminator_cfg["hidden_dims"],
            reward_scale=self.discriminator_cfg["reward_scale"],
            device=self.device,
            loss_type=self.discriminator_cfg["loss_type"],
            empirical_normalization=self.discriminator_cfg["empirical_normalization"],
        ).to(self.device)

        # åˆå§‹åŒ– PPO ç®—æ³•
        alg_class = eval(self.alg_cfg.pop("class_name"))  # e.g., AMP_PPO
        
        # æ¸…ç† alg_cfgï¼šç§»é™¤é‚£äº›åœ¨ AMP_PPO ä¸­ä¸å­˜åœ¨ä½†åœ¨æ ‡å‡† rsl_rl PPO ä¸­å­˜åœ¨çš„å‚æ•°ï¼Œé˜²æ­¢æŠ¥é”™
        # (ä¾‹å¦‚ RND, Symmetry, Multi-GPU config ç­‰)
        for key in list(self.alg_cfg.keys()):
            if key not in AMP_PPO.__init__.__code__.co_varnames:
                self.alg_cfg.pop(key)

        # å®ä¾‹åŒ– AMP_PPO ç®—æ³•
        alg: AMP_PPO = alg_class(
            actor_critic=actor_critic,
            discriminator=self.discriminator,
            amp_data=amp_data,
            device=self.device,
            **self.alg_cfg,
        )
        return alg