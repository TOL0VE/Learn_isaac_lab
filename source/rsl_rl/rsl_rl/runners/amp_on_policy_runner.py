# Copyright (c) 2025, Istituto Italiano di Tecnologia
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause


from __future__ import annotations

import os
import time
import torch
import statistics
from collections import deque
from tensordict import TensorDict

import rsl_rl
from rsl_rl.env import VecEnv
from rsl_rl.modules import ActorCritic, ActorCriticRecurrent
from rsl_rl.utils import resolve_obs_groups
from rsl_rl.utils.logger import Logger  # [Upgrade] ä½¿ç”¨å®˜æ–¹ Logger

from rsl_rl.utils import AMPLoader
from rsl_rl.algorithms import AMP_PPO
from rsl_rl.networks import Discriminator, ActorCriticMoE
from rsl_rl.utils import export_policy_as_onnx


class AMPOnPolicyRunner:
    """
    AMPOnPolicyRunner æ˜¯ä¸€ä¸ªé«˜çº§åè°ƒå™¨ï¼Œç”¨äºç®¡ç†ä½¿ç”¨å¯¹æŠ—è¿åŠ¨å…ˆéªŒ (AMP) 
    ç»“åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹  (PPO) çš„ç­–ç•¥è®­ç»ƒå’Œè¯„ä¼°ã€‚

    å®ƒæ•´åˆäº†å¤šä¸ªç»„ä»¶ï¼š
    - ç¯å¢ƒ (`VecEnv`)
    - ç­–ç•¥ (`ActorCritic`, `ActorCriticRecurrent`)
    - åˆ¤åˆ«å™¨ (Discriminator)
    - ä¸“å®¶æ•°æ®é›† (AMPLoader)
    - å¥–åŠ±ç»„åˆ (ä»»åŠ¡å¥–åŠ± + é£æ ¼å¥–åŠ±)
    - æ—¥å¿—è®°å½•å’Œæ£€æŸ¥ç‚¹ä¿å­˜

    ---
    ğŸ”§ é…ç½® Configuration
    ----------------
    è¯¥ç±»æœŸæœ›ä¸€ä¸ª `train_cfg` å­—å…¸ï¼Œå…¶ç»“æ„åŒ…å«ä»¥ä¸‹é”®ï¼š
    - "obs_groups": å¯é€‰æ˜ å°„ï¼Œæè¿°å“ªäº›è§‚æµ‹å¼ é‡å±äº "policy" è¾“å…¥ï¼Œå“ªäº›å±äº "critic" è¾“å…¥ã€‚
    - "policy": ç­–ç•¥ç½‘ç»œçš„é…ç½®ï¼ŒåŒ…æ‹¬ `"class_name"`ã€‚
    - "algorithm": PPO/AMP_PPO çš„é…ç½®ï¼ŒåŒ…æ‹¬ `"class_name"`ã€‚
    - "discriminator": AMP åˆ¤åˆ«å™¨çš„é…ç½®ã€‚
    - "dataset": ä¼ é€’ç»™ `AMPLoader` çš„å­—å…¸ï¼Œè‡³å°‘åŒ…å«ï¼š
        * "amp_data_path": å­˜æ”¾ `.npy` ä¸“å®¶æ•°æ®é›†çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
        * "datasets": æ•°æ®é›†åç§° -> é‡‡æ ·æƒé‡ (float) çš„æ˜ å°„ã€‚
        * "slow_down_factor": åº”ç”¨äºçœŸå®è¿åŠ¨æ•°æ®çš„å‡é€Ÿå› å­ï¼Œä»¥åŒ¹é…ä»¿çœŸåŠ¨åŠ›å­¦ã€‚
    - "num_steps_per_env": æ¯ä¸ªç¯å¢ƒçš„ Rollout è§†ç•Œé•¿åº¦ (horizon)ã€‚
    - "save_interval": æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ï¼ˆä»¥è¿­ä»£æ¬¡æ•°è®¡ï¼‰ã€‚
    - "empirical_normalization": (å·²å¼ƒç”¨) é•œåƒåˆ° `policy.actor_obs_normalization` çš„æ—§æ ‡å¿—ã€‚
    - "logger": "tensorboard", "wandb", æˆ– "neptune" ä¹‹ä¸€ã€‚

    ---
    ğŸ“¦ æ•°æ®é›†æ ¼å¼ Dataset format
    ------------------
    é€šè¿‡ `AMPLoader` åŠ è½½çš„ä¸“å®¶è¿åŠ¨æ•°æ®é›†å¿…é¡»æ˜¯åŒ…å«å­—å…¸çš„ `.npy` æ–‡ä»¶ï¼š

    - `"joints_list"`: List[str] â€” æœ‰åºçš„å…³èŠ‚åç§°åˆ—è¡¨ã€‚
    - `"joint_positions"`: List[np.ndarray] â€” æ¯ä¸ªæ—¶é—´æ­¥çš„å…³èŠ‚é…ç½® (1D æ•°ç»„)ã€‚
    - `"root_position"`: List[np.ndarray] â€” ä¸–ç•Œåæ ‡ç³»ä¸‹çš„åŸºåº§ä½ç½®ã€‚
    - `"root_quaternion"`: List[np.ndarray] â€” **`xyzw`** æ ¼å¼çš„åŸºåº§æ–¹å‘ (SciPy é»˜è®¤)ã€‚
    - `"fps"`: float â€” åŸå§‹æ•°æ®é›†å¸§ç‡ã€‚

    å†…éƒ¨å¤„ç†ï¼š
    - å››å…ƒæ•°é€šè¿‡ SLERP æ’å€¼å¹¶åœ¨ä½¿ç”¨å‰è½¬æ¢ä¸º **`wxyz`** æ ¼å¼ (ä»¥åŒ¹é… Isaac Gym æƒ¯ä¾‹)ã€‚
    - é€Ÿåº¦é€šè¿‡æœ‰é™å·®åˆ†ä¼°ç®—ã€‚
    - æ‰€æœ‰æ•°æ®è½¬æ¢ä¸º torch tensors å¹¶æ”¾ç½®åœ¨æŒ‡å®šè®¾å¤‡ä¸Šã€‚

    ---
    ğŸ“ AMP å¥–åŠ± AMP Reward
    -------------
    åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œrunner æ”¶é›† AMP ç‰¹å®šçš„è§‚æµ‹ï¼Œå¹¶ä»ä¸“å®¶æ•°æ®é›†ä¸­è®¡ç®—
    åŸºäºåˆ¤åˆ«å™¨çš„â€œé£æ ¼å¥–åŠ±â€ã€‚è¯¥å¥–åŠ±ä¸ç¯å¢ƒå¥–åŠ±ç»“åˆå¦‚ä¸‹ï¼š

        `reward = 0.5 * task_reward + 0.5 * style_reward`
    
    (æ³¨æ„ï¼šä»£ç å®é™…å®ç°ä¸­é€šå¸¸æ˜¯åœ¨ Env æˆ– Wrapper é‡Œåšæ··åˆï¼Œæˆ–è€…åƒè¿™é‡Œä¸€æ ·åœ¨ Runner çš„ step å¾ªç¯é‡Œæ··åˆ)

    ---
    ğŸ” è®­ç»ƒå¾ªç¯ Training loop
    ----------------
    `learn()` æ–¹æ³•æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
    - `rollout`: é€šè¿‡ `self.alg.act()` å’Œ `env.step()` æ”¶é›† TensorDict è§‚æµ‹ã€‚
    - `style_reward`: é€šè¿‡åˆ¤åˆ«å™¨ `predict_reward(...)` è®¡ç®—ã€‚
    - `storage update`: é€šè¿‡ `process_env_step()` å’Œ `process_amp_step()` æ›´æ–°å­˜å‚¨ã€‚
    - `return computation`: ä½¿ç”¨æœ€æ–°çš„ TensorDict è§‚æµ‹é€šè¿‡ `compute_returns()` è®¡ç®—å›æŠ¥ã€‚
    - `update`: ä½¿ç”¨ `self.alg.update()` æ‰§è¡Œåå‘ä¼ æ’­ã€‚
    - é€šè¿‡ TensorBoard/WandB/Neptune è®°å½•æ—¥å¿—ã€‚

    ---
    ğŸ’¾ ä¿å­˜å’Œ ONNX å¯¼å‡º Saving and ONNX export
    --------------------------
    åœ¨æ¯ä¸ª `save_interval`ï¼Œrunner ä¼šï¼š
    - ä¿å­˜å®Œæ•´çŠ¶æ€ (`model`, `optimizer`, `discriminator`, ç­‰)ã€‚
    - å¯é€‰åœ°å°†ç­–ç•¥å¯¼å‡ºä¸º ONNX æ¨¡å‹ç”¨äºéƒ¨ç½²ã€‚
    - å¦‚æœå¯ç”¨ï¼Œå°†æ£€æŸ¥ç‚¹ä¸Šä¼ åˆ°æ—¥å¿—æœåŠ¡ã€‚

    [Upgrade Note]: è¿™æ˜¯ä¸€ä¸ªå‡çº§ç‰ˆï¼Œé›†æˆäº† rsl_rl v2.x çš„é«˜çº§æ—¥å¿—ç³»ç»Ÿå’Œå¤š GPU æ”¯æŒã€‚
    """

    def __init__(self, env: VecEnv, train_cfg: dict, log_dir: str | None = None, device: str = "cpu"):
        self.cfg = train_cfg
        self.alg_cfg = train_cfg["algorithm"]
        self.policy_cfg = train_cfg["policy"]
        self.discriminator_cfg = train_cfg["discriminator"]
        self.dataset_cfg = train_cfg["dataset"]
        self.device = device
        self.env = env

        # [Upgrade] é…ç½®å¤š GPU è®­ç»ƒ
        self._configure_multi_gpu()

        # è·å–ç¯å¢ƒè§‚æµ‹
        observations = self.env.get_observations()
        default_sets = ["critic"]
        
        # [Upgrade] è§£æè§‚æµ‹åˆ†ç»„
        self.cfg["obs_groups"] = resolve_obs_groups(
            observations, self.cfg.get("obs_groups"), default_sets
        )

        # æ„å»ºç®—æ³• (ActorCritic, Discriminator, PPO)
        self.alg: AMP_PPO = self._construct_algorithm(observations)
        
        self.num_steps_per_env = self.cfg["num_steps_per_env"]
        self.save_interval = self.cfg["save_interval"]
        
        # åˆå§‹åŒ– Storage (Rollout Buffer)
        obs_template = observations.clone().detach().to(self.device)
        self.alg.init_storage(
            self.env.num_envs,
            self.num_steps_per_env,
            obs_template,
            (self.env.num_actions,),
        )

        # [Upgrade] åˆå§‹åŒ–é«˜çº§ Logger
        self.logger = Logger(
            log_dir=log_dir,
            cfg=self.cfg,
            env_cfg=self.env.cfg,
            num_envs=self.env.num_envs,
            is_distributed=self.is_distributed,
            gpu_world_size=self.gpu_world_size,
            gpu_global_rank=self.gpu_global_rank,
            device=self.device,
        )

        self.current_learning_iteration = 0

    def learn(self, num_learning_iterations: int, init_at_random_ep_len: bool = False):
        """æ‰§è¡Œä¸»è®­ç»ƒå¾ªç¯ã€‚"""
        
        # éšæœºåŒ–åˆå§‹å›åˆé•¿åº¦
        if init_at_random_ep_len:
            self.env.episode_length_buf = torch.randint_like(
                self.env.episode_length_buf, high=int(self.env.max_episode_length)
            )
            
        # è·å–åˆå§‹è§‚æµ‹
        obs = self.env.get_observations().to(self.device)
        # è·å–åˆå§‹ AMP è§‚æµ‹ (ç”¨äºè®¡ç®— reward)
        amp_obs = obs["amp"].clone() if "amp" in obs else torch.zeros(1, device=self.device) # é˜²å¾¡æ€§ç¼–ç¨‹
        
        self.train_mode()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼

        # [Upgrade] å¤š GPU åŒæ­¥å‚æ•°
        if self.is_distributed:
            print(f"Synchronizing parameters for rank {self.gpu_global_rank}...")
            # æ³¨æ„ï¼šå¦‚æœä½ çš„ AMP_PPO è¿˜æ²¡å®ç° broadcast_parametersï¼Œè¿™é‡Œå¯èƒ½ä¼šæŠ¥é”™
            # ä½†æ ‡å‡†çš„ rsl_rl PPO æ˜¯æœ‰çš„ï¼Œä½ å¯ä»¥æš‚æ—¶æ³¨é‡Šæ‰æˆ–è€…å»å®ç°å®ƒ
            # self.alg.broadcast_parameters() 
            pass

        # ç»Ÿè®¡æ•°æ®ç¼“å­˜ (ä»…ç”¨äºè®¡ç®— AMP Reward çš„ loggingï¼Œå…¶ä»–ç”± Logger æ¥ç®¡)
        # å®é™…ä¸Š Logger ä¹Ÿä¼šå¤„ç† rewbufferï¼Œè¿™é‡Œä¸»è¦ä¸ºäº†è®¡ç®— mean_style_reward
        start_iter = self.current_learning_iteration
        total_iter = start_iter + num_learning_iterations
        
        # >>> ä¸»å¾ªç¯å¼€å§‹ <<<
        for it in range(start_iter, total_iter):
            start = time.time()
            
            # --- 1. Rollout (æ•°æ®æ”¶é›†) ---
            mean_style_reward_log = 0.0
            mean_task_reward_log = 0.0

            with torch.inference_mode():
                for _ in range(self.num_steps_per_env):
                    # 1. ç­–ç•¥åŠ¨ä½œ
                    actions = self.alg.act(obs)
                    self.alg.act_amp(amp_obs)
                    
                    # 2. ç¯å¢ƒæ­¥è¿›
                    obs, rewards, dones, extras = self.env.step(actions.to(self.env.device))
                    obs, rewards, dones = obs.to(self.device), rewards.to(self.device), dones.to(self.device)

                    # 3. AMP é€»è¾‘
                    next_amp_obs = obs["amp"].clone() # ç¡®ä¿ DirectEnv è¿”å›äº†è¿™ä¸ª key
                    style_rewards = self.discriminator.predict_reward(amp_obs, next_amp_obs)

                    # è®°å½•åŸå§‹å¥–åŠ± (Logç”¨)
                    mean_task_reward_log += rewards.mean().item()
                    mean_style_reward_log += style_rewards.mean().item()

                    # 4. æ··åˆå¥–åŠ± (Task + Style)
                    total_rewards = 0.5 * rewards + 0.5 * style_rewards

                    # 5. å¤„ç†æ•°æ®
                    self.alg.process_env_step(obs, total_rewards, dones, extras)
                    self.alg.process_amp_step(next_amp_obs)
                    
                    amp_obs = next_amp_obs

                    # [Upgrade] Logger å¤„ç†æ­¥è¿›ä¿¡æ¯
                    # æ³¨æ„ï¼šæˆ‘ä»¬è¿™é‡Œä¼ å…¥çš„æ˜¯æ··åˆåçš„ total_rewards è¿˜æ˜¯åŸå§‹ rewardsï¼Ÿ
                    # é€šå¸¸ Log é‡Œçœ‹ Task Reward æ›´æœ‰æ„ä¹‰ï¼Œä½† PPO ä¼˜åŒ–çš„æ˜¯ Totalã€‚
                    # è¿™é‡Œä¸ºäº†å…¼å®¹æ ‡å‡† Loggerï¼Œæˆ‘ä»¬ä¼ æ··åˆåçš„ Totalï¼Œæˆ–è€…ä½ å¯ä»¥é­”æ”¹ Logger ä¼  tupleã€‚
                    # RSL-RL Logger é»˜è®¤åªè®°å½•ä¼ å…¥çš„ rewardsã€‚
                    self.logger.process_env_step(rewards, dones, extras) # è®°å½•åŸå§‹ Task Reward æ¯”è¾ƒç›´è§‚

                stop = time.time()
                collection_time = stop - start
                start = stop

                # è®¡ç®— GAE å›æŠ¥
                self.alg.compute_returns(obs)

            # å½’ä¸€åŒ– log æ•°æ®
            mean_style_reward_log /= self.num_steps_per_env
            mean_task_reward_log /= self.num_steps_per_env

            # --- 2. Update (å­¦ä¹ æ›´æ–°) ---
            loss_dict = self.alg.update()
            
            # [Upgrade] å°† AMP ç‰¹æœ‰çš„ Loss ä¹Ÿå¡è¿› loss_dict
            # AMP_PPO.update() è¿”å›çš„æ˜¯ tupleï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒè½¬æ¢æˆ dict ä»¥å–‚ç»™ Logger
            # å‡è®¾ä½ çš„ AMP_PPO.update è¿”å›çš„æ˜¯ tuple (å¦‚ä½ ä¹‹å‰ä»£ç æ‰€ç¤º)
            # æˆ‘ä»¬æ‰‹åŠ¨è§£åŒ…å¹¶æ„å»º dict
            (
                val_loss, surr_loss, amp_loss, grad_pen, 
                pol_pred, exp_pred, acc_pol, acc_exp, kl
            ) = loss_dict
            
            # é‡æ„ä¸ºå­—å…¸ä¾› Logger ä½¿ç”¨
            loss_dict_log = {
                # PPO æ ¸å¿ƒæŸå¤±
                "Loss/value_function": val_loss,
                "Loss/surrogate": surr_loss,
                "Loss/kl_divergence": kl,
                "Policy/learning_rate": self.alg.learning_rate,
                
                # AMP åˆ¤åˆ«å™¨æŸå¤±
                "AMP/discriminator_loss": amp_loss,
                "AMP/grad_penalty": grad_pen,
                
                # AMP é¢„æµ‹å€¼ (è¶Šé«˜è¶Šåƒæ˜¯ä¸“å®¶)
                "AMP/pred_policy": pol_pred,  # å‡æ•°æ®çš„å¾—åˆ† (ç›®æ ‡æ˜¯è®©å®ƒå˜é«˜)
                "AMP/pred_expert": exp_pred,  # çœŸæ•°æ®çš„å¾—åˆ† (é€šå¸¸å¾ˆé«˜)
                
                # AMP å‡†ç¡®ç‡ (è¶Šä½è¶Šå¥½ï¼Œè¯´æ˜åˆ¤åˆ«å™¨è¢«éª—äº†)
                "AMP/accuracy_policy": acc_pol, # åˆ¤åˆ«å™¨è¯†åˆ«å‡æ•°æ®çš„å‡†ç¡®ç‡
                "AMP/accuracy_expert": acc_exp, # åˆ¤åˆ«å™¨è¯†åˆ«çœŸæ•°æ®çš„å‡†ç¡®ç‡
                
                # å¥–åŠ±æˆåˆ† (ç›‘æ§ Task vs Style çš„æ¯”ä¾‹)
                "Reward/style": mean_style_reward_log,
                "Reward/task": mean_task_reward_log,
            }

            stop = time.time()
            learn_time = stop - start
            self.current_learning_iteration = it
            
            # --- 3. Logging (æ—¥å¿—) ---
            # [Upgrade] ä½¿ç”¨é«˜çº§ Logger
            self.logger.log(
                it=it,
                start_it=start_iter,
                total_it=total_iter,
                collect_time=collection_time,
                learn_time=learn_time,
                loss_dict=loss_dict_log,
                learning_rate=self.alg.learning_rate,
                action_std=self.alg.actor_critic.action_std,
                rnd_weight=self.alg.rnd.weight if self.alg_cfg["rnd_cfg"] else None,
            )
            
            # --- 4. Saving (ä¿å­˜) ---
            if it % self.save_interval == 0:
                self.save(os.path.join(self.logger.log_dir, f"model_{it}.pt"))

        # è®­ç»ƒç»“æŸä¿å­˜
        if self.logger.log_dir is not None and not self.logger.disable_logs:
            self.save(os.path.join(self.logger.log_dir, f"model_{self.current_learning_iteration}.pt"))

    def save(self, path: str, infos: dict | None = None) -> None:
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
        saved_dict = {
            "model_state_dict": self.alg.actor_critic.state_dict(),
            "optimizer_state_dict": self.alg.optimizer.state_dict(),
            "discriminator_state_dict": self.alg.discriminator.state_dict(),
            "iter": self.current_learning_iteration,
            "infos": infos,
        }
        torch.save(saved_dict, path)

        # [Upgrade] Logger è´Ÿè´£ä¸Šä¼ äº‘ç«¯
        self.logger.save_model(path, self.current_learning_iteration)

        # å°è¯•å¯¼å‡º ONNX (å¯é€‰)
        try:
            onnx_path = os.path.dirname(path)
            onnx_name = f"policy_{self.current_learning_iteration}.onnx"
            export_policy_as_onnx(
                self.alg.actor_critic,
                normalizer=self.alg.actor_critic.actor_obs_normalizer,
                path=onnx_path,
                filename=onnx_name,
            )
        except Exception as e:
            print(f"ONNX export failed: {e}")

    def load(self, path: str, load_optimizer: bool = True, map_location: str | None = None) -> dict:
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
        loaded_dict = torch.load(path, map_location=map_location, weights_only=False)
        self.alg.actor_critic.load_state_dict(loaded_dict["model_state_dict"])
        self.alg.discriminator.load_state_dict(loaded_dict["discriminator_state_dict"], strict=False)
        
        # å…¼å®¹æ—§ç‰ˆæœ¬
        if "amp_normalizer" in loaded_dict and self.discriminator_cfg.get("empirical_normalization", False):
            self.alg.discriminator.amp_normalizer.load_state_dict(loaded_dict["amp_normalizer"].state_dict())
            
        if load_optimizer:
            self.alg.optimizer.load_state_dict(loaded_dict["optimizer_state_dict"])
            
        self.current_learning_iteration = loaded_dict["iter"]
        return loaded_dict["infos"]

    def get_inference_policy(self, device: str | None = None) -> callable:
        """è·å–ç”¨äºæ¨ç†çš„ç­–ç•¥å‡½æ•°ã€‚"""
        self.eval_mode()
        if device is not None:
            self.alg.actor_critic.to(device)
        return self.alg.actor_critic.act_inference

    def train_mode(self):
        self.alg.actor_critic.train()
        self.alg.discriminator.train()

    def eval_mode(self):
        self.alg.actor_critic.eval()
        self.alg.discriminator.eval()

    def add_git_repo_to_log(self, repo_file_path: str) -> None:
        self.logger.git_status_repos.append(repo_file_path)

    def _configure_multi_gpu(self) -> None:
        """[Upgrade] Configure multi-gpu training."""
        self.gpu_world_size = int(os.getenv("WORLD_SIZE", "1"))
        self.is_distributed = self.gpu_world_size > 1

        if not self.is_distributed:
            self.gpu_local_rank = 0
            self.gpu_global_rank = 0
            return

        self.gpu_local_rank = int(os.getenv("LOCAL_RANK", "0"))
        self.gpu_global_rank = int(os.getenv("RANK", "0"))

        if self.device != f"cuda:{self.gpu_local_rank}":
            # å¦‚æœä¸æƒ³å¼ºåˆ¶æŠ¥é”™ï¼Œå¯ä»¥è¿™é‡Œ print ä¸€ä¸ª warning
            # raise ValueError(f"Device mismatch: {self.device} vs cuda:{self.gpu_local_rank}")
            pass

        torch.distributed.init_process_group(backend="nccl", rank=self.gpu_global_rank, world_size=self.gpu_world_size)
        torch.cuda.set_device(self.gpu_local_rank)

    def _construct_algorithm(self, observations: TensorDict) -> AMP_PPO:
        # 1. åˆå§‹åŒ– Policy (ActorCritic / MoE)
        actor_critic_class = eval(self.policy_cfg.pop("class_name"))
        actor_critic = actor_critic_class(
            observations,
            self.cfg["obs_groups"],
            self.env.num_actions,
            **self.policy_cfg,
        ).to(self.device)
        
        # 2. è·å–å…³èŠ‚åç§° (å…¼å®¹ Direct / Manager)
        if hasattr(self.env.unwrapped, "cfg") and hasattr(self.env.unwrapped.cfg, "dof_names"):
             amp_joint_names = self.env.unwrapped.cfg.dof_names
        elif hasattr(self.env.unwrapped, "robot") and hasattr(self.env.unwrapped.robot.data, "joint_names"):
            amp_joint_names = self.env.unwrapped.robot.data.joint_names
        elif hasattr(self.env.cfg, "observations") and hasattr(self.env.cfg.observations, "amp"):
            amp_joint_names = self.env.cfg.observations.amp.joint_pos.params["asset_cfg"].joint_names
        elif "amp_joint_names" in self.dataset_cfg:
             amp_joint_names = self.dataset_cfg["amp_joint_names"]
        else:
            raise AttributeError("Could not find joint names for AMPLoader.")

        # 3. è·å– AMP è§‚æµ‹ç»´åº¦
        if hasattr(self.env.unwrapped, "amp_observation_size"):
            num_amp_obs = self.env.unwrapped.amp_observation_size
        else:
            raise AttributeError("Define 'self.amp_observation_size' in Env.")
        
        # 4. åˆå§‹åŒ– AMP Loader
        amp_data = AMPLoader(
            env=self.env, # ç›´æ¥æŠŠç¯å¢ƒä¼ è¿›å»
            device=self.device,
            time_between_frames=self.env.cfg.sim.dt * self.env.cfg.decimation, # dt
        )

        # 5. åˆå§‹åŒ– Discriminator
        self.discriminator = Discriminator(
            input_dim=num_amp_obs * 2,
            hidden_layer_sizes=self.discriminator_cfg["hidden_dims"],
            reward_scale=self.discriminator_cfg["reward_scale"],
            device=self.device,
            loss_type=self.discriminator_cfg["loss_type"],
            empirical_normalization=self.discriminator_cfg["empirical_normalization"],
        ).to(self.device)

        alg_cfg_copy = self.alg_cfg.copy()
        # 6. åˆå§‹åŒ– AMP_PPO
        alg_class = eval(alg_cfg_copy.pop("class_name"))
        
        # æ¸…ç†å¤šä½™å‚æ•°
        for key in list(alg_cfg_copy.keys()):
            if key not in AMP_PPO.__init__.__code__.co_varnames:
                alg_cfg_copy.pop(key)

        # å®ä¾‹åŒ– AMP_PPO ç®—æ³•
        alg: AMP_PPO = alg_class(
            actor_critic=actor_critic,
            discriminator=self.discriminator,
            amp_data=amp_data,
            device=self.device,
            **alg_cfg_copy,
        )
        return alg