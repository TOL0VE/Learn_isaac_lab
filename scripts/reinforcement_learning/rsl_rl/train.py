# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Script to train RL agent with RSL-RL."""

"""Launch Isaac Sim Simulator first."""

import argparse
import sys

# AppLauncher æ˜¯ Isaac Lab ç”¨æ¥å¯åŠ¨ Omniverse ä»¿çœŸå™¨æ ¸å¿ƒçš„å·¥å…·ç±»
from isaaclab.app import AppLauncher

# local imports
# cli_args æ˜¯æœ¬åœ°çš„ä¸€ä¸ªè¾…åŠ©æ¨¡å—ï¼Œç”¨æ¥å¤„ç†è·Ÿ RSL-RL ç›¸å…³çš„ç‰¹å®šå‚æ•°
import cli_args  # isort: skip

# -----------------------------------------------------------------------------
# 1. å®šä¹‰å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
# -----------------------------------------------------------------------------
parser = argparse.ArgumentParser(description="Train an RL agent with RSL-RL.")

# --video: å¼€å…³ï¼Œæ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å½•åˆ¶è§†é¢‘
parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
# --video_length: å½•åˆ¶çš„è§†é¢‘é•¿åº¦ï¼ˆæ­¥æ•°ï¼‰ï¼Œé»˜è®¤å½• 200 æ­¥
parser.add_argument("--video_length", type=int, default=200, help="Length of the recorded video (in steps).")
# --video_interval: æ¯éš”å¤šå°‘æ­¥å½•ä¸€æ¬¡è§†é¢‘ï¼Œé»˜è®¤ 2000 æ­¥
parser.add_argument("--video_interval", type=int, default=2000, help="Interval between video recordings (in steps).")

# --num_envs: å¹¶è¡Œç¯å¢ƒçš„æ•°é‡ã€‚å¦‚æœæŒ‡å®šäº†ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶é‡Œçš„é»˜è®¤å€¼ã€‚
parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
# --task: ä»»åŠ¡åç§°ï¼ˆä¾‹å¦‚ "Isaac-Cartpole-v0"ï¼‰ï¼Œè¿™æ˜¯å¿…é¡»æŒ‡å®šçš„ï¼Œç”¨æ¥å» Gym æ³¨å†Œè¡¨é‡Œæ‰¾ç¯å¢ƒ
parser.add_argument("--task", type=str, default=None, help="Name of the task.")
# --agent: ç®—æ³•é…ç½®çš„å…¥å£ç‚¹åç§°ï¼Œé»˜è®¤ä¸º "rsl_rl_cfg_entry_point"
# è¿™å¯¹åº”äº† gym.register æ—¶ kwargs é‡Œæ³¨å†Œçš„é‚£ä¸ª key
parser.add_argument(
    "--agent", type=str, default="rsl_rl_cfg_entry_point", help="Name of the RL agent configuration entry point."
)
# --seed: éšæœºç§å­ï¼Œç”¨äºå¤ç°å®éªŒç»“æœ
parser.add_argument("--seed", type=int, default=None, help="Seed used for the environment")
# --max_iterations: è®­ç»ƒçš„æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆPPO çš„å¤§å¾ªç¯æ¬¡æ•°ï¼‰
parser.add_argument("--max_iterations", type=int, default=None, help="RL Policy training iterations.")
# --distributed: æ˜¯å¦å¼€å¯å¤š GPU åˆ†å¸ƒå¼è®­ç»ƒ
parser.add_argument(
    "--distributed", action="store_true", default=False, help="Run training with multiple GPUs or nodes."
)
# --export_io_descriptors: æ˜¯å¦å¯¼å‡º I/O æè¿°æ–‡ä»¶ï¼ˆç”¨äº Warp ç­‰é«˜çº§ç‰¹æ€§ï¼Œä¸€èˆ¬ä¸ç”¨ï¼‰
parser.add_argument("--export_io_descriptors", action="store_true", default=False, help="Export IO descriptors.")
# --ray-proc-id: Ray æ¡†æ¶ä½¿ç”¨çš„è¿›ç¨‹ IDï¼Œå¦‚æœä¸ç”¨ Ray è°ƒå‚å¯ä»¥å¿½ç•¥
parser.add_argument(
    "--ray-proc-id", "-rid", type=int, default=None, help="Automatically configured by Ray integration, otherwise None."
)

# æ·»åŠ  RSL-RL ç‰¹æœ‰çš„å‚æ•°ï¼ˆæ¯”å¦‚ --resume, --run_name ç­‰ï¼‰
cli_args.add_rsl_rl_args(parser)

# æ·»åŠ  AppLauncher ç‰¹æœ‰çš„å‚æ•°ï¼ˆæ¯”å¦‚ --headless, --device ç­‰ï¼‰
AppLauncher.add_app_launcher_args(parser)

# è§£æå‚æ•°
# parse_known_args å…è®¸ä¼ å…¥ä¸€äº› parser ä¸è®¤è¯†çš„å‚æ•°ï¼ˆè¿™äº›å¤šä½™çš„å‚æ•°ä¼šè¢« hydra æ¡èµ°ï¼‰
args_cli, hydra_args = parser.parse_known_args()

# å¦‚æœå¼€å¯äº†è§†é¢‘å½•åˆ¶ï¼Œå¼ºåˆ¶å¼€å¯ç›¸æœºåŠŸèƒ½
if args_cli.video:
    args_cli.enable_cameras = True

# æ¸…ç† sys.argvï¼Œåªä¿ç•™è„šæœ¬åå’Œ hydra éœ€è¦çš„å‚æ•°
# è¿™æ˜¯ä¸ºäº†é˜²æ­¢ argparse å¤„ç†è¿‡çš„å‚æ•°å¹²æ‰°åç»­çš„ Hydra é…ç½®åŠ è½½
sys.argv = [sys.argv[0]] + hydra_args

# -----------------------------------------------------------------------------
# 2. å¯åŠ¨ Omniverse ä»¿çœŸå™¨
# -----------------------------------------------------------------------------
# åˆå§‹åŒ– AppLauncherï¼Œè¿™ä¼šè¯»å– --headless ç­‰å‚æ•°å¹¶é…ç½® Kit
app_launcher = AppLauncher(args_cli)
# çœŸæ­£çš„å¯åŠ¨ï¼è¿™ä¸€è¡Œæ‰§è¡Œåï¼ŒIsaac Sim çš„æ ¸å¿ƒæ‰è¢«åŠ è½½ï¼Œæ‰èƒ½ import pxr/omni ç­‰åº“
simulation_app = app_launcher.app

"""Check for minimum supported RSL-RL version."""

import importlib.metadata as metadata
import platform

from packaging import version

# å®šä¹‰æ‰€éœ€çš„æœ€ä½ rsl-rl ç‰ˆæœ¬
RSL_RL_VERSION = "3.0.1"
# è·å–å½“å‰ç¯å¢ƒå®‰è£…çš„ç‰ˆæœ¬
installed_version = metadata.version("rsl-rl-lib")

# å¦‚æœç‰ˆæœ¬å¤ªä½ï¼Œæ‰“å°æŠ¥é”™ä¿¡æ¯å¹¶é€€å‡º
if version.parse(installed_version) < version.parse(RSL_RL_VERSION):
    if platform.system() == "Windows":
        cmd = [r".\isaaclab.bat", "-p", "-m", "pip", "install", f"rsl-rl-lib=={RSL_RL_VERSION}"]
    else:
        cmd = ["./isaaclab.sh", "-p", "-m", "pip", "install", f"rsl-rl-lib=={RSL_RL_VERSION}"]
    print(
        f"Please install the correct version of RSL-RL.\nExisting version is: '{installed_version}'"
        f" and required version is: '{RSL_RL_VERSION}'.\nTo install the correct version, run:"
        f"\n\n\t{' '.join(cmd)}\n"
    )
    exit(1)


    """Rest everything follows."""

import gymnasium as gym
import logging
import os
import time
import torch
from datetime import datetime

# å¯¼å…¥ RSL-RL çš„ Runner (æ‰§è¡Œè®­ç»ƒå¾ªç¯çš„æ ¸å¿ƒç±»)
# OnPolicyRunner ç”¨äº PPOï¼ŒDistillationRunner ç”¨äºè’¸é¦
from rsl_rl.runners import DistillationRunner, OnPolicyRunner

# å¯¼å…¥ Isaac Lab çš„ç¯å¢ƒç›¸å…³ç±»
from isaaclab.envs import (
    DirectMARLEnv,
    DirectMARLEnvCfg,
    DirectRLEnvCfg,
    ManagerBasedRLEnvCfg,
    multi_agent_to_single_agent, # è¾…åŠ©å‡½æ•°ï¼šæŠŠå¤šæ™ºèƒ½ä½“ç¯å¢ƒåŒ…è£…æˆå•æ™ºèƒ½ä½“æ¥å£
)
from isaaclab.utils.dict import print_dict
from isaaclab.utils.io import dump_yaml

# å¯¼å…¥ Isaac Lab å¯¹ RSL-RL çš„é€‚é…å™¨
from isaaclab_rl.rsl_rl import RslRlBaseRunnerCfg, RslRlVecEnvWrapper

# å¯¼å…¥ isaaclab_tasks ä»¥ä¾¿æ³¨å†Œæ‰€æœ‰ä»»åŠ¡ (gym.make èƒ½æ‰¾åˆ°å®ƒä»¬)
import isaaclab_tasks  # noqa: F401
from isaaclab_tasks.utils import get_checkpoint_path
# è¿™æ˜¯ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºä»æ³¨å†Œè¡¨åŠ è½½ Hydra é…ç½®
from isaaclab_tasks.utils.hydra import hydra_task_config

# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# PLACEHOLDER: Extension template (do not remove this comment)

# -----------------------------------------------------------------------------
# PyTorch æ€§èƒ½ä¼˜åŒ–è®¾ç½®
# -----------------------------------------------------------------------------
# å…è®¸ä½¿ç”¨ TF32 æ ¼å¼ï¼ˆåœ¨ Ampere æ¶æ„ GPU ä¸ŠåŠ é€ŸçŸ©é˜µä¹˜æ³•ï¼‰
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
# å…³é—­ç¡®å®šæ€§è®¡ç®—ï¼ˆä¸ºäº†é€Ÿåº¦ï¼Œå¦‚æœéœ€è¦å®Œå…¨å¤ç°æ€§åº”è®¾ä¸º Trueï¼‰
torch.backends.cudnn.deterministic = False
torch.backends.cudnn.benchmark = False


# è£…é¥°å™¨ä½œç”¨ï¼š
# 1. æ‹¦æˆª main è°ƒç”¨
# 2. æ ¹æ® args_cli.task æ‰¾åˆ°ä»»åŠ¡é…ç½® -> env_cfg
# 3. æ ¹æ® args_cli.task å’Œ args_cli.agent æ‰¾åˆ°ç®—æ³•é…ç½® -> agent_cfg
# 4. æŠŠè¿™ä¸¤ä¸ªå¯¹è±¡ä¼ ç»™ main å‡½æ•°
@hydra_task_config(args_cli.task, args_cli.agent)
def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlBaseRunnerCfg):
    """Train with RSL-RL agent."""
    
    # -------------------------------------------------------------------------
    # 1. é…ç½®è¦†ç›– (Configuration Override)
    # -------------------------------------------------------------------------
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶é‡Œçš„è®¾ç½®ï¼ˆæ¯”å¦‚ä½ å‘½ä»¤è¡Œæ•²äº† --seed 42ï¼Œå°±è¦è¦†ç›– yaml é‡Œçš„ seedï¼‰
    agent_cfg = cli_args.update_rsl_rl_cfg(agent_cfg, args_cli)
    
    # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº† --num_envsï¼Œè¦†ç›–ç¯å¢ƒé…ç½®
    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs
    
    # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº† --max_iterationsï¼Œè¦†ç›–ç®—æ³•é…ç½®
    agent_cfg.max_iterations = (
        args_cli.max_iterations if args_cli.max_iterations is not None else agent_cfg.max_iterations
    )

    # -------------------------------------------------------------------------
    # 2. éšæœºç§å­ä¸è®¾å¤‡è®¾ç½® (Seed & Device)
    # -------------------------------------------------------------------------
    # åŒæ­¥ç¯å¢ƒå’Œç®—æ³•çš„ç§å­
    env_cfg.seed = agent_cfg.seed
    # è®¾ç½®è¿è¡Œè®¾å¤‡ (cuda:0 æˆ– cpu)
    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device
    
    # æ£€æŸ¥ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¸æ”¯æŒ CPU
    if args_cli.distributed and args_cli.device is not None and "cpu" in args_cli.device:
        raise ValueError(
            "Distributed training is not supported when using CPU device. "
            "Please use GPU device (e.g., --device cuda) for distributed training."
        )

    # -------------------------------------------------------------------------
    # 3. åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½® (Multi-GPU)
    # -------------------------------------------------------------------------
    if args_cli.distributed:
        # æ ¹æ®æœ¬åœ° rank è®¾ç½®è®¾å¤‡ï¼ˆæ¯”å¦‚è¿›ç¨‹ 0 ç”¨ cuda:0ï¼Œè¿›ç¨‹ 1 ç”¨ cuda:1ï¼‰
        env_cfg.sim.device = f"cuda:{app_launcher.local_rank}"
        agent_cfg.device = f"cuda:{app_launcher.local_rank}"

        # è°ƒæ•´ç§å­ï¼šä¸åŒè¿›ç¨‹å¿…é¡»ç”¨ä¸åŒçš„ç§å­ï¼Œå¦åˆ™æ‰€æœ‰ GPU é‡‡æ ·çš„ç»éªŒéƒ½ä¸€æ ·ï¼Œè®­ç»ƒå°±åºŸäº†
        seed = agent_cfg.seed + app_launcher.local_rank
        env_cfg.seed = seed
        agent_cfg.seed = seed

    # -------------------------------------------------------------------------
    # 4. æ—¥å¿—ç›®å½•è®¾ç½® (Logging)
    # -------------------------------------------------------------------------
    # æ ¹ç›®å½•ï¼šlogs/rsl_rl/å®éªŒå
    log_root_path = os.path.join("logs", "rsl_rl", agent_cfg.experiment_name)
    log_root_path = os.path.abspath(log_root_path)
    print(f"[INFO] Logging experiment in directory: {log_root_path}")
    
    # å­ç›®å½•ï¼šæ—¶é—´æˆ³_è¿è¡Œå (ä¾‹å¦‚ 2024-01-01_12-00-00_run1)
    log_dir = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    print(f"Exact experiment name requested from command line: {log_dir}")
    if agent_cfg.run_name:
        log_dir += f"_{agent_cfg.run_name}"
    log_dir = os.path.join(log_root_path, log_dir)

    # -------------------------------------------------------------------------
    # 5. I/O æè¿°ç¬¦å¯¼å‡º (å¯¹äº Manager-Based ç¯å¢ƒ)
    # -------------------------------------------------------------------------
    if isinstance(env_cfg, ManagerBasedRLEnvCfg):
        env_cfg.export_io_descriptors = args_cli.export_io_descriptors
    else:
        # Direct æ¨¡å¼ä¸æ”¯æŒè¿™ä¸ªï¼ˆä¹Ÿä¸éœ€è¦ï¼‰
        logger.warning(
            "IO descriptors are only supported for manager based RL environments. No IO descriptors will be exported."
        )

    # å°†è®¡ç®—å¥½çš„æ—¥å¿—ç›®å½•å¡å›ç¯å¢ƒé…ç½®é‡Œ
    env_cfg.log_dir = log_dir

    # -------------------------------------------------------------------------
    # 6. åˆ›å»ºç¯å¢ƒ (Create Environment)
    # -------------------------------------------------------------------------
    # gym.make ä¼šè°ƒç”¨ isaaclab.envs:ManagerBasedRLEnv æˆ– DirectRLEnv
    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)

    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœåº•å±‚æ˜¯å¤šæ™ºèƒ½ä½“ç¯å¢ƒ (MARL)ï¼Œä½†æˆ‘ä»¬è¦ç”¨å•æ™ºèƒ½ä½“ç®—æ³•è·‘ (PPO)
    # è¿™ä¸ª wrapper ä¼šæŠŠæ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹æ‹¼èµ·æ¥
    if isinstance(env.unwrapped, DirectMARLEnv):
        env = multi_agent_to_single_agent(env)

    # -------------------------------------------------------------------------
    # 7. æ£€æŸ¥ç‚¹æ¢å¤è·¯å¾„ (Checkpoint Resume)
    # -------------------------------------------------------------------------
    # å¦‚æœæŒ‡å®šäº† --resumeï¼Œè®¡ç®—ä¹‹å‰çš„æ¨¡å‹è·¯å¾„
    if agent_cfg.resume or agent_cfg.algorithm.class_name == "Distillation":
        resume_path = get_checkpoint_path(log_root_path, agent_cfg.load_run, agent_cfg.load_checkpoint)

    # -------------------------------------------------------------------------
    # 8. è§†é¢‘å½•åˆ¶åŒ…è£…å™¨ (Video Recording Wrapper)
    # -------------------------------------------------------------------------
    if args_cli.video:
        video_kwargs = {
            "video_folder": os.path.join(log_dir, "videos", "train"),
            "step_trigger": lambda step: step % args_cli.video_interval == 0, # æ¯éš” interval æ­¥å½•ä¸€æ¬¡
            "video_length": args_cli.video_length, # å½•å¤šé•¿
            "disable_logger": True,
        }
        print("[INFO] Recording videos during training.")
        print_dict(video_kwargs, nesting=4)
        # ä½¿ç”¨ Gym æ ‡å‡†çš„ RecordVideo wrapper
        env = gym.wrappers.RecordVideo(env, **video_kwargs)

    start_time = time.time()

    # -------------------------------------------------------------------------
    # 9. RSL-RL ç¯å¢ƒåŒ…è£…å™¨ (RSL-RL Wrapper)
    # -------------------------------------------------------------------------
    # RSL-RL æœŸæœ›ç¯å¢ƒè¿”å›çš„æ˜¯ torch.Tensorï¼Œå¹¶ä¸”ä½äº GPU ä¸Š
    # RslRlVecEnvWrapper è´Ÿè´£æŠŠ Gym çš„æ¥å£è½¬æ¢æˆ RSL-RL å–œæ¬¢çš„æ¥å£
    # å¹¶ä¸”å¤„ç† clip_actions (æŠŠç½‘ç»œè¾“å‡ºè£å‰ªåˆ° [-1, 1])
    env = RslRlVecEnvWrapper(env, clip_actions=agent_cfg.clip_actions)

    # -------------------------------------------------------------------------
    # 10. åˆ›å»º Runner å¹¶å¼€å§‹è®­ç»ƒ (Create Runner & Learn)
    # -------------------------------------------------------------------------
    # æ ¹æ®é…ç½®ç±»åé€‰æ‹© Runner
    if agent_cfg.class_name == "OnPolicyRunner":
        # æ ‡å‡† PPO è®­ç»ƒå™¨
        runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)
    elif agent_cfg.class_name == "DistillationRunner":
        # è’¸é¦è®­ç»ƒå™¨
        runner = DistillationRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)
    else:
        raise ValueError(f"Unsupported runner class: {agent_cfg.class_name}")
    
    # è®°å½•å½“å‰ä»£ç çš„ git hash åˆ°æ—¥å¿—ï¼Œæ–¹ä¾¿ä»¥åæŸ¥ç‰ˆæœ¬
    runner.add_git_repo_to_log(__file__)
    
    # å¦‚æœæ˜¯ Resume æ¨¡å¼ï¼ŒåŠ è½½æ¨¡å‹æƒé‡
    if agent_cfg.resume or agent_cfg.algorithm.class_name == "Distillation":
        print(f"[INFO]: Loading model checkpoint from: {resume_path}")
        runner.load(resume_path)

    # æŠŠæœ€ç»ˆç”Ÿæ•ˆçš„å‚æ•°ä¿å­˜ä¸º YAMLï¼Œæ–¹ä¾¿å¤æŸ¥
    dump_yaml(os.path.join(log_dir, "params", "env.yaml"), env_cfg)
    dump_yaml(os.path.join(log_dir, "params", "agent.yaml"), agent_cfg)

    # ğŸš€ å¼€å§‹è®­ç»ƒï¼
    # init_at_random_ep_len=True: è®­ç»ƒå¼€å§‹æ—¶ï¼ŒéšæœºåŒ–æ¯ä¸ªç¯å¢ƒçš„å½“å‰æ­¥æ•°
    # è¿™å¯ä»¥é˜²æ­¢æ‰€æœ‰ç¯å¢ƒåŒæ—¶ Resetï¼Œå¯¼è‡´æ•°æ®åˆ†å¸ƒå‡ºç°å‘¨æœŸæ€§æ³¢åŠ¨
    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)

    print(f"Training time: {round(time.time() - start_time, 2)} seconds")

    # -------------------------------------------------------------------------
    # 11. æ¸…ç†å·¥ä½œ (Cleanup)
    # -------------------------------------------------------------------------
    # å…³é—­ç¯å¢ƒå’Œä»¿çœŸå™¨
    env.close()

if __name__ == "__main__":
    # run the main function
    main()
    # close sim app
    simulation_app.close()